\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}
\usepackage{listings}
% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\usepackage{xcolor}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{
style=mystyle
}

\title{Bayesian Personalized Ranking \\
DSC 210 
}

\author{Joel, Lora, Antiriksha, Jemin}

  
\begin{document}
\maketitle

\section{Matrix Representation and Factorization}
We can represent users and items as matrices in a latent feature space.\\
In the user feature matrix we would have rows correspond to a user and the columns would represent features such as User ID, Item Purchased, Rating given, etc.. 
\begin{center}
Users: $U = \{u_1, u_2, u_3,..., u_n\}$\\
Items: $I = \{i_1, i_2, i_3,..., i_m\}$ \\
Features: $F = \{f_1, f_2, f_3, ..., f_n\}$
\end{center}
User feature Matrix (P): \\
\begin{center}
U = $\begin{bmatrix}
    u_1 \\ u_2 \\ u_3 \\ \vdots \\ u_n
\end{bmatrix}$
F = $\begin{bmatrix}
    f_1 & f _2 & f_3 & \hdots & f_n
\end{bmatrix}$ \\~\\
P = UF \\
P =
$Users\underbrace{\begin{bmatrix}
    u_1f_1 & u_1f_2 & u_1f_3 & \hdots &  u_1f_n\\ 
    u_2f_1 & u_2f_2 & u_2f_3 & \hdots &  u_2f_n\\
    u_3f_1 & u_3f_2 & u_3f_3 & \hdots &  u_3f_n\\ 
    \vdots & . & . & . & .\\
    u_nf_1 & . & . & \hdots & u_nf_n
\end{bmatrix}}_{Features} = \begin{bmatrix} p_{u1} \\ p_{u2} \\ p_{u3} \\ \vdots \\ p_{un} \end{bmatrix}$   \\~\\

Item feature Matrix (Q): \\

I = $\begin{bmatrix}
    i_1 \\ i_2 \\ i_3 \\ \vdots \\ i_n
\end{bmatrix}$
F = $\begin{bmatrix}
    f_1 & f _2 & f_3 & \hdots & f_n
\end{bmatrix}$ \\~\\
Q = IF \\
Q =
$Items\underbrace{\begin{bmatrix}
    i_1f_1 & i_1f_2 & i_1f_3 & \hdots &  i_1f_n\\ 
    i_2f_1 & i_2f_2 & i_2f_3 & \hdots &  i_2f_n\\
    i_3f_1 & i_3f_2 & i_3f_3 & \hdots &  i_3f_n\\ 
    \vdots & . & . & . & .\\
    i_mf_1 & . & . & \hdots & i_mf_n
\end{bmatrix}}_{Features} = \begin{bmatrix} q_{i1} \\ q_{i2} \\ q_{i3} \\ \vdots \\ q_{im} \end{bmatrix}$   \\~\\

\( p_{ui} \) and \( q_{ij} \) are latent factors representing the user and item characteristics, respectively.

\end{center}

\section{Loss Function}
The goal of BPR is to optimize the ranking of items for each user. In order to do this we take user (u) and see the rating score for item (i) is positive or negative. \\
We are looking for prediction scores where: \\
\begin{center}
    \(i_{positive} > i_{negative}\) \\~\\
\end{center}
The loss function can be represented by:
\begin{center}
    \(-\ln(\sigma(\hat{y}{ui_{positive}} - \hat{y}{ui_{negative}}))\) \\~\\
    \(\hat{y}_{ui} = p_u \cdot q_i \) ; predicted score \(\hat{y}\), for user \(u\) and item \(i\)\\
    \( \sigma(x) = \frac{1}{1 + e^{-x}} \) ; maps the difference between the positive prediction and the negative prediction to a value between 0 and 1
\end{center}

\section{Training}
Effectively, our goal is to minimize the loss function with our model by adjusting the latent factors of P and Q to learn what a users preference may be. 

\end{document}

